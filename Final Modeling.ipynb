{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "duplicate-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "np.random.seed(42)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features containing all 0's\n",
    "train_byte_unigram = pd.read_csv('train/bytes_ngrams.csv')\n",
    "train_hed_unigram = pd.read_csv('train/header_ngrams.csv')\n",
    "train_unigrams = pd.merge(train_byte_unigram, train_hed_unigram, on='file')\n",
    "\n",
    "test_byte_unigram = pd.read_csv('test/bytes_ngrams.csv')\n",
    "test_hed_unigram = pd.read_csv('test/header_ngrams.csv')\n",
    "test_unigrams = pd.merge(test_byte_unigram, test_hed_unigram, on='file')\n",
    "\n",
    "joined_unigrams = train_unigrams.append(test_unigrams)\n",
    "joined_unigrams = joined_unigrams.loc[:, (joined_unigrams != 0).any(axis=0)]\n",
    "\n",
    "train_unigrams = joined_unigrams.iloc[:10868, :].copy()\n",
    "test_unigrams = joined_unigrams.iloc[10868:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nasty-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10868, 303), (10873, 303))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unigrams.shape, test_unigrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "covered-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features containing all 1's\n",
    "train_pixels = pd.read_csv('train/asm_pixels.csv')\n",
    "train_pixels.rename(columns={'0':'file'}, inplace=True)\n",
    "\n",
    "test_pixels = pd.read_csv('test/asm_pixels.csv')\n",
    "test_pixels.rename(columns={'0':'file'}, inplace=True)\n",
    "\n",
    "joined_pixels = train_pixels.append(test_pixels)\n",
    "joined_pixels = joined_pixels.loc[:, (joined_pixels != 1.0).any(axis=0)]\n",
    "\n",
    "train_pixels = joined_pixels.iloc[:10868, :].copy()\n",
    "test_pixels = joined_pixels.iloc[10868:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "separate-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10868, 601), (10873, 601))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pixels.shape, test_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "animal-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features containing all 0's\n",
    "train_opc = pd.read_csv('train/opcodes_ngrams.csv')\n",
    "\n",
    "test_opc = pd.read_csv('test/opcodes_ngrams.csv')\n",
    "\n",
    "joined_opc = train_opc.append(test_opc)\n",
    "joined_opc = joined_opc.loc[:, (joined_opc != 0).any(axis=0)]\n",
    "\n",
    "train_opc = joined_opc.iloc[:10868, :].copy()\n",
    "test_opc = joined_opc.iloc[10868:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "violent-silicon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10868, 12376), (10873, 12376))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_opc.shape, test_opc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assisted-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting all features by filenames and concatenating to create train set \n",
    "labels = pd.read_csv('trainLabels.csv')\n",
    "labels.columns = ['file', 'label']\n",
    "labels.sort_values('file', inplace=True)\n",
    "y = labels['label'].values\n",
    "\n",
    "train_asm_size = pd.read_csv('train/asm_size.csv')\n",
    "train_byte_size = pd.read_csv('train/byte_size.csv')\n",
    "train_size = pd.merge(train_asm_size, train_byte_size, on='file')\n",
    "train_size.sort_values('file', inplace=True)\n",
    "train_size.drop('file', axis=1, inplace=True)\n",
    "\n",
    "train_unigrams.sort_values('file', inplace=True)\n",
    "train_unigrams.drop('file', axis=1, inplace=True)\n",
    "select_unigrams = SelectPercentile(chi2, percentile=20)\n",
    "train_unigrams = select_unigrams.fit_transform(train_unigrams, y)\n",
    "\n",
    "train_pixels.sort_values('file', inplace=True)\n",
    "train_pixels.drop('file', axis=1, inplace=True)\n",
    "\n",
    "train_opc.sort_values('file', inplace=True)\n",
    "train_opc.drop('file', axis=1, inplace=True)\n",
    "select_opc = SelectPercentile(chi2, percentile=5)\n",
    "train_opc = select_opc.fit_transform(train_opc, y)\n",
    "\n",
    "X = np.hstack((train_size.values, train_unigrams, train_pixels.values, train_opc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-secretary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10868, 1282), (10868,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spectacular-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting all features by filenames and concatenating to create test set \n",
    "test_asm_size = pd.read_csv('test/asm_size.csv')\n",
    "test_byte_size = pd.read_csv('test/byte_size.csv')\n",
    "test_size = pd.merge(test_asm_size, test_byte_size, on='file')\n",
    "test_size.sort_values('file', inplace=True)\n",
    "test_files = test_size['file']\n",
    "test_size.drop('file', axis=1, inplace=True)\n",
    "\n",
    "test_unigrams.sort_values('file', inplace=True)\n",
    "test_unigrams.drop('file', axis=1, inplace=True)\n",
    "test_unigrams = select_unigrams.transform(test_unigrams)\n",
    "\n",
    "test_pixels.sort_values('file', inplace=True)\n",
    "test_pixels.drop('file', axis=1, inplace=True)\n",
    "\n",
    "test_opc.sort_values('file', inplace=True)\n",
    "test_opc.drop('file', axis=1, inplace=True)\n",
    "test_opc = select_opc.transform(test_opc)\n",
    "\n",
    "test_set = np.hstack((test_size.values, test_unigrams, test_pixels.values, test_opc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nearby-south",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10873, 1282)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "checked-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(y) #xgboost only accepts labels from 0 to n-1 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distant-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Params:  {'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.5}\n",
      "Best CV score: 0.02259655021742208\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "xgb = xgboost.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', alpha=1, eta=0.1, use_label_encoder=False, n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "     'n_estimators': [500, 750, 1000],\n",
    "     'max_depth': [3, 5, 7, 9],\n",
    "    'colsample_bytree':[0.5, 0.7, 0.9]}\n",
    "\n",
    "cv = RandomizedSearchCV(xgb, param_grid, cv=5, scoring='neg_log_loss', verbose=1, n_iter=20)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', cv.best_params_)\n",
    "print(f'Best CV score: {-cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "south-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train log loss is: 0.0036004091717198035\n",
      "The test log loss is: 0.015118401389574238\n"
     ]
    }
   ],
   "source": [
    "y_hat = cv.predict_proba(X_train)\n",
    "print (\"The train log loss is:\", log_loss(y_train, y_hat))\n",
    "y_hat = cv.predict_proba(X_test)\n",
    "print(\"The test log loss is:\", log_loss(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monthly-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = cv.predict_proba(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "advance-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating submissio file for kaggle\n",
    "sub = pd.DataFrame(test_files.values, columns=['ID'])\n",
    "sub[['Prediction1', 'Prediction2', 'Prediction3', 'Prediction4', 'Prediction5', 'Prediction6', 'Prediction7', 'Prediction8', 'Prediction9']] = test_pred\n",
    "sub.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaggle test score 0.02170; Rank 83\n",
    "#Should try ensembling more models to improve score"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}